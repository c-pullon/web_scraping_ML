{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b696fc23",
   "metadata": {},
   "source": [
    "### Wikipedia Web Scraping, Data Analysis and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c4ec9",
   "metadata": {},
   "source": [
    "This project involves using Beautiful Soup to extract data from Wikipedia's list of top 50 largest companies by revenue.\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue\n",
    "\n",
    "Following successful web scraping and formatting into a DataFrame, exploratory data analysis is performed to better understand the distributions and relatinships in the data. \n",
    "\n",
    "Increasingly more complex classification models are then built to classify whether a company is state-owned or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f4abe",
   "metadata": {},
   "source": [
    "### Part 1: web scraping, data formatting and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700242c0",
   "metadata": {},
   "source": [
    "#### Web scraping and column identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25843bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue'\n",
    "\n",
    "# Use the requests library to send a GET request to the URL to retrieve the HTML content\n",
    "page = requests.get(url)\n",
    "\n",
    "# Parse the HTML with Beautiful Soup to create a soup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# Find all tables on the wikipedia page and select the first one\n",
    "table = soup.find_all('table')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the header row of the specified table and iterate over each table header (<th>) element \n",
    "# within the header row and extract the text using the get_text() method. Strip whitespace from \n",
    "# the text and append the cleaned text to the column_names list.\n",
    "header_row = table.find('tr')\n",
    "column_names = []\n",
    "\n",
    "for header in header_row.find_all('th'):\n",
    "    header_text = header.get_text(strip=True)\n",
    "    column_names.append(header_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c2e70",
   "metadata": {},
   "source": [
    "#### Row and data identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = []\n",
    "\n",
    "# Iterate over each row (<tr>), starting from the third row (index 2) to skip the header rows.\n",
    "for row in table.find_all('tr')[2:]:\n",
    "    data_row = [] # Within each iteration of the outer loop,initialise an empty list to store the data for the current row.\n",
    "    for cell in row.find_all(['td', 'th']):\n",
    "        cell_text = cell.get_text(strip=True)\n",
    "        # Check if the cell contains an image, and check whether the alt text for the image is 'Yes' or 'No'\n",
    "        img = cell.find('img')\n",
    "        if img:\n",
    "            if img['alt'] == 'Yes': \n",
    "                cell_text = 'Yes'\n",
    "            elif img['alt'] == 'No':\n",
    "                cell_text = 'No'\n",
    "        if cell_text != 'USD millions':\n",
    "            data_row.append(cell_text)\n",
    "    data_rows.append(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5fa568",
   "metadata": {},
   "source": [
    "#### DataFrame creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15697414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame and add the columns and data rows.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_rows, columns=column_names)\n",
    "\n",
    "# Display DataFrame\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5560ef",
   "metadata": {},
   "source": [
    "#### DataFrame formatting and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4be181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some formatting and cleaning is required here: the 'Ref.' column is not needed and the 'Headquarters[note 1]' column is\n",
    "# renamed for clarity. The 'Revenue' and 'Profit' columns need to be formatted to remove the '$' so they can be converted to\n",
    "# integers. Commas in columns containing numbers are also removed to make analysis easier.\n",
    "\n",
    "\n",
    "# Drop the 'Ref.' column\n",
    "df.drop(columns=['Ref.'], inplace=True)\n",
    "\n",
    "\n",
    "# Rename the 'Headquarters[note 1]' column to 'Headquarters'\n",
    "df.rename(columns={'Headquarters[note 1]': 'Headquarters'}, inplace=True)\n",
    "\n",
    "\n",
    "# Remove leading '$' from 'Revenue', 'Profit', and 'Revenue per worker' columns\n",
    "df['Revenue'] = df['Revenue'].str.replace('$', '', regex=False)\n",
    "df['Profit'] = df['Profit'].str.replace('$', '', regex=False)\n",
    "df['Revenue per worker'] = df['Revenue per worker'].str.replace('$', '', regex=False)\n",
    "\n",
    "\n",
    "# Remove commas from numeric columns\n",
    "df['Profit'] = df['Profit'].str.replace(',', '')\n",
    "df['Revenue'] = df['Revenue'].str.replace(',', '')\n",
    "df['Employees'] = df['Employees'].str.replace(',', '')\n",
    "df['Revenue per worker'] = df['Revenue per worker'].str.replace(',', '')\n",
    "\n",
    "\n",
    "# Convert the 'Profit' column to integers, skipping non-convertible values\n",
    "df['Profit'] = pd.to_numeric(df['Profit'], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "# Convert the 'Revenue' column to integers\n",
    "df['Revenue'] = df['Revenue'].astype(int)\n",
    "\n",
    "\n",
    "# Convert the 'Employees' column to integers\n",
    "df['Employees'] = df['Employees'].astype(int)\n",
    "\n",
    "\n",
    "# Convert the 'Revenue per worker' column to integers\n",
    "df['Revenue per worker'] = df['Revenue per worker'].astype(float)\n",
    "\n",
    "# Rename columns to specify units\n",
    "df.rename(columns={'Revenue': 'Revenue (USD millions)', 'Profit': 'Profit (USD millions)', 'Revenue per worker': 'Revenue per worker (USD)'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV with explicit encoding\n",
    "df.to_csv('companies_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f60e1a",
   "metadata": {},
   "source": [
    "### Part 2: exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910999c",
   "metadata": {},
   "source": [
    "#### Histogram creation to visualise distribution of numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35edff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['int32', 'Int64', 'float64'])\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(nrows=len(numerical_columns.columns), ncols=1, figsize=(10, 5*len(numerical_columns.columns)))\n",
    "\n",
    "# Loop through each numerical column\n",
    "for i, col in enumerate(numerical_columns.columns):\n",
    "    # Histogram\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Histogram of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a629ac",
   "metadata": {},
   "source": [
    "#### Scatter plot of revenue and profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd05e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter out rows with NA revenue\n",
    "filtered_df = df.dropna(subset=['Profit (USD millions)'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_df['Revenue (USD millions)'], filtered_df['Profit (USD millions)'], alpha=0.5)\n",
    "plt.title('Scatter Plot of Revenue vs Profit')\n",
    "plt.xlabel('Revenue (USD millions)')\n",
    "plt.ylabel('Profit (USD millions)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e2fe9",
   "metadata": {},
   "source": [
    "#### Calculate the pearson correlation coefficient, ignoring NA rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58169b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Filter the DataFrame to include only the columns of interest\n",
    "filtered_df = df[['Revenue (USD millions)', 'Profit (USD millions)']].copy()\n",
    "\n",
    "# Drop rows with any NaN values in the selected columns\n",
    "filtered_df.dropna(subset=['Revenue (USD millions)', 'Profit (USD millions)'], inplace=True)\n",
    "\n",
    "# Calculate the correlation coefficient using pearsonr\n",
    "correlation, _ = pearsonr(filtered_df['Revenue (USD millions)'], filtered_df['Profit (USD millions)'])\n",
    "print(\"Correlation coefficient:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecedd52b",
   "metadata": {},
   "source": [
    "For companies with lower profit and revenue, it seems there is a stronger correlation between these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759627ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows where revenue is 200000 or lower\n",
    "low_revenue_df = df[df['Revenue (USD millions)'] <= 220000].copy()\n",
    "\n",
    "# Drop rows with missing values\n",
    "low_revenue_df.dropna(subset=['Revenue (USD millions)', 'Profit (USD millions)'], inplace=True)\n",
    "\n",
    "# Calculate the correlation coefficient for this subset\n",
    "correlation_low_revenue, _ = pearsonr(low_revenue_df['Revenue (USD millions)'], low_revenue_df['Profit (USD millions)'])\n",
    "print(\"Correlation coefficient for companies with revenue of 220000 million USD or lower:\", correlation_low_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print list of companies with a 'low' revenue\n",
    "\n",
    "print(low_revenue_df[['Name', 'Revenue (USD millions)', 'Profit (USD millions)']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe53f08",
   "metadata": {},
   "source": [
    "#### Checking the number of state-owned vs privately-owned companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_owned_counts = df['State-owned'].value_counts()\n",
    "print(\"Number of state-owned companies:\", state_owned_counts['Yes'])\n",
    "print(\"Number of private companies:\", state_owned_counts['No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23973f6",
   "metadata": {},
   "source": [
    "### Part 3: Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73325842",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac49ca",
   "metadata": {},
   "source": [
    "I want to see whether a model can accurately predict whether a company is state-owned based on its numeric features. We can see that there are over double the number of private companies to state-owned, so when splitting the data into training and test sets, the split is stratified to keep the proportion of state-owned companies the same in each set. This eliminates sample bias and reflects the population being studied. \n",
    "\n",
    "Logistic regression is first used to classify the data into state-owned or not state-owned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Drop rows where profit is NaN\n",
    "df.dropna(subset=['Profit (USD millions)'], inplace=True)\n",
    "\n",
    "# Assume X contains your features and y contains the target variable (state-owned or not)\n",
    "X = df[['Revenue (USD millions)', 'Profit (USD millions)', 'Employees', 'Revenue per worker (USD)']]\n",
    "y = df['State-owned']\n",
    "\n",
    "# Split the data into training and testing sets, stratifying by the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Display the proportions of state-owned and private companies in the training set\n",
    "print(\"Training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Display the proportions of state-owned and private companies in the testing set\n",
    "print(\"\\nTesting set:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"\\nAccuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8dd42",
   "metadata": {},
   "source": [
    "These poor results show that just guessing a company is not state-owned would be better, as most of the companies are not state-owned. A more balanced dataset may produce better results. \n",
    "\n",
    "Furthermore, the features used for classification may not be important or the relationships may not be linear. Logistic regression assumes linear relationships between the input variables and the target variable; if these relationships are not linear then logistic regression would naturally perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4013a30",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91383bd0",
   "metadata": {},
   "source": [
    "Random forest will now be used for classification because it is capable of capturing non-linear relationships between features in data. This may lead to improved performance from logistic regression if the relationhips are non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a858a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Classifier Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f09241",
   "metadata": {},
   "source": [
    "These improved results suggest the the relationships may be non-linear, although the model still performs poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf40e87",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102210e3",
   "metadata": {},
   "source": [
    "The final classification model used is XGBoost. This is a powerful gradient-boosting algorithm which has gained popularity for being efficient and accurate. Here a grid search of hyperparameter values is performed to find the best combination from a defined list. An XGBoost model is trained and tested using these values, and the performance is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define parameters for the XGBoost model\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',  \n",
    "    'eval_metric': 'error',           \n",
    "}\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Define the dataset\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_encoded)\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.3, 0.6],       \n",
    "    'max_depth': [4, 6],                 \n",
    "    'min_child_weight': [1, 3],          \n",
    "    'subsample': [0.8, 1.0],                \n",
    "    'n_estimators': [50, 100],\n",
    "    'gamma': [0, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [0, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params_xgb = grid_search.best_params_\n",
    "print(\"Best Parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "# Train the XGBoost model with the best parameters\n",
    "best_xgb_model = xgb.XGBClassifier(**xgb_params, **best_params_xgb)\n",
    "best_xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Decode the predictions\n",
    "y_pred_test_decoded = label_encoder.inverse_transform(y_pred_test_xgb)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_test_decoded)\n",
    "print(\"Test Accuracy (XGBoost):\", accuracy_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9070dd1",
   "metadata": {},
   "source": [
    "These results are improved over random forest, but still not very good. It is encouraging to see successive models getting better results, but accuracy may be limited due to a number of reasons.\n",
    "\n",
    ". There may be more influential features which are not included in the model, in the future it may be possible to add extra information for these companies.\n",
    "\n",
    ". The hyperparameter set may not be optimal; while grid search was used to find the best combination, there are many more adjustable hyperparameters and values to test.\n",
    "\n",
    ". The dataset is small. In the future, a much larger dataset with thousands or more companies can be used, which may improve results for all models tested.\n",
    "\n",
    ". There are companies which have an NA value for profit. I decided to remove these from the model, but replacing them with something like the mean profit of the other companies, or just '0' would increase the size of the dataset and potentially improve results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
